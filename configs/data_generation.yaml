# ------------------------------------------------Global Paths------------------------------------------------#
# Paths to models
model2path:
  Llama-3.2-3B-Instruct: "models/Llama-3.2-3B-Instruct"


save_dir: "log/"

# -------------------------------------------------Generator Settings------------------------------------------------#
framework: vllm # inference frame work of LLM, supporting: 'hf','vllm','fschat'
generator_model: "Qwen2.5-72B-Instruct" # name or path of the generator model
generator_max_input_len: 8192  # max length of the input
generator_batch_size: 1 # batch size for generation, invalid for vllm
gpu_memory_utilization: 0.9
generation_params:
  do_sample: True
  max_tokens: 1024
  # qwen 72b
  # temperature: 0.7
  # top_p: 0.8

  # deepseek 70b
  # temperature: 0.6
  # top_p: 0.95
  # top_k: -1

  # llama 3B
  temperature: 0.6
  top_p: 0.9
use_fid: False # whether to use FID, only valid in encoder-decoder model
