# ------------------------------------------------Global Paths------------------------------------------------#
# Paths to models
model2path:
  Llama-3.1-8B-Instruct: "meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659"


save_dir: "log/"

# -------------------------------------------------Generator Settings------------------------------------------------#
framework: vllm # inference frame work of LLM, supporting: 'hf','vllm','fschat'
generator_model: "Llama-3.1-8B-Instruct" # name or path of the generator model
generator_max_input_len: 8192  # max length of the input
generator_batch_size: 1 # batch size for generation, invalid for vllm
gpu_memory_utilization: 0.9
generation_params:
  do_sample: True
  max_tokens: 1024
  # qwen 72b
  # temperature: 0.7
  # top_p: 0.8

  # deepseek 70b
  # temperature: 0.6
  # top_p: 0.95
  # top_k: -1

  # llama 3B
  temperature: 0.6
  top_p: 0.9
use_fid: False # whether to use FID, only valid in encoder-decoder model
